{
  "id": "Pygeoflood-Container",
  "version": "0.0.36",
  "description": "Runs pygeoflood in a container.",
  "owner": "${apiUserId}",
  "enabled": true,
  "runtime": "SINGULARITY",
  "runtimeVersion": null,
  "runtimeOptions": ["SINGULARITY_RUN"],
  "containerImage": "docker://ghcr.io/tobiashi26/pygeoflood-container:main",
  "jobType": "BATCH",
  "maxJobs": -1,
  "maxJobsPerUser": -1,
  "strictFileInputs": true,
  "jobAttributes": {
    "description": null,
    "dynamicExecSystem": false,
    "execSystemConstraints": null,
    "execSystemId": "ls6",
    "execSystemExecDir": "${JobWorkingDir}",
    "execSystemInputDir": "${JobWorkingDir}",
    "execSystemOutputDir": "${JobWorkingDir}/output",
    "execSystemLogicalQueue": "development",
    "archiveSystemId": "cloud.data",
    "archiveSystemDir": "HOST_EVAL($HOME)/tapis-jobs-archive/${JobCreateDate}/${JobName}-${JobUUID}",
    "archiveOnAppError": true,
    "isMpi": false,
    "mpiCmd": null,
    "cmdPrefix": "mkdir $PWD/work $PWD/home $PWD/scratch;",
    "parameterSet": {
      "appArgs": [
        {
          "name": "DEM",
          "description": "The name of the DEM being used (example names could be Original if unmodified DEM, Dike1 if dike is addded to DEM, Reservoir_expansion if reservoir is expanded)",
          "inputMode": "REQUIRED",
          "arg": "Original"
        },
        {
          "name": "Points",
          "description": "The name of the DEM being used (example names could be Original if unmodified DEM, Dike1 if dike is addded to DEM, Reservoir_expansion if reservoir is expanded)",
          "inputMode": "REQUIRED",
          "arg": "None"
        },
        {
          "name": "Flood Map List",
          "description": "The name of the DEM being used (example names could be Original if unmodified DEM, Dike1 if dike is addded to DEM, Reservoir_expansion if reservoir is expanded)",
          "inputMode": "REQUIRED",
          "arg": "All"
        }
      ],
      "schedulerOptions": [
        {
          "name": "TACC Scheduler Profile",
          "description": "Scheduler profile for HPC clusters at TACC",
          "inputMode": "FIXED",
          "arg": "--tapis-profile tacc-apptainer",
          "notes": {
            "isHidden": true
          }
        },
        {
          "name": "TAP Session Substring",
          "description": "TAP Functions require the substring 'tap_' and in the slurm job name in order to function.",
          "inputMode": "FIXED",
          "arg": "--job-name ${JobName}-tap_",
          "notes": {
            "isHidden": true
          }
        }
      ],
      "envVariables": [],
      "archiveFilter": {
        "includes": [],
        "excludes": [],
        "includeLaunchFiles": true
      }
    },
    "fileInputs": [
      {
        "name": "Catchment Folder",
        "description": "Contains the billing data to be processed",
        "inputMode": "REQUIRED",
        "autoMountLocal": true,
        "sourceUrl": null,
        "targetPath": "Catchment"
      }
    ],
    "fileInputArrays": [],
    "nodeCount": 1,
    "coresPerNode": 1,
    "memoryMB": 1000,
    "maxMinutes": 10,
    "subscriptions": [],
    "tags": []
  },
  "tags": ["portalName: ALL"],
  "notes": {
    "label": "Python script using conda",
    "helpUrl": "https://github.com/In-For-Disaster-Analytics/cookbook-conda-template",
    "helpText": "Read CSV file stored on TACC storage and run a Python script that reads it, calculates the average of the values in the first column, and writes the result to a file.",
    "hideNodeCountAndCoresPerNode": true,
    "isInteractive": false,
    "icon": "jupyter",
    "category": "Data Processing",
    "queueFilter": ["development", "normal"]
  }
}
